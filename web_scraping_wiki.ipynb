{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-scraping-com-Python\" data-toc-modified-id=\"Web-scraping-com-Python-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web scraping com Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introdução\" data-toc-modified-id=\"Introdução-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introdução</a></span></li></ul></li><li><span><a href=\"#Conceitos-básicos\" data-toc-modified-id=\"Conceitos-básicos-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Conceitos básicos</a></span><ul class=\"toc-item\"><li><span><a href=\"#HTML\" data-toc-modified-id=\"HTML-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>HTML</a></span></li><li><span><a href=\"#CSS\" data-toc-modified-id=\"CSS-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>CSS</a></span></li><li><span><a href=\"#JS\" data-toc-modified-id=\"JS-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>JS</a></span></li><li><span><a href=\"#Hands-On\" data-toc-modified-id=\"Hands-On-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Hands On</a></span><ul class=\"toc-item\"><li><span><a href=\"#Explicando-como-extraimos-a-explicação-da-Wikipedia\" data-toc-modified-id=\"Explicando-como-extraimos-a-explicação-da-Wikipedia-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Explicando como extraimos a explicação da Wikipedia</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Matheus de Vasconcellos Barroso\n",
    "\n",
    "# Web scraping com Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "Aprenderemos alguns conceitos básicos de **web scraping** e como utilizar o [Python](https://www.python.org/) para essa tarefa. Mais precisamente esse material foi preparado utilizando um [Jupyter Notebook](https://jupyter.org/) e algumas extensões úteis ([`nbextensions`](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)).\n",
    "O exemplo que utilizaremos será a definição de [**web scraping**](https://en.wikipedia.org/wiki/Web_scraping) pela [Wikipedia](https://www.wikipedia.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"650\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Web_scraping\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x902e0f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from IPython.display import display, HTML\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src = 'https://en.wikipedia.org/wiki/Web_scraping', width = 900, height=650)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine como seria útil desenvolver uma técnica para ler o primeiro ou **n** primeiros parágrafos de uma página da [Wikipedia](https://www.wikipedia.org/) para testar algorítmos de sumarização? Utilizando as técnicas de raspagem de dados podemos obter de forma simples os três primeiros parágrafos da url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.\n",
      "\n",
      "Web scraping a web page involves fetching it and extracting from it.[1][2] Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies and their URLs, to a list (contact scraping).\n",
      "\n",
      "Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup and, web data integration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Web_scraping\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "for item in soup.find_all('p')[:3]: print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para alcançar esse objetivo final serão abordados alguns tópicos necessários para compreender como funciona esse processo:\n",
    "+ [HTML](https://en.wikipedia.org/wiki/HTML)\n",
    "+ [CSS](https://en.wikipedia.org/wiki/CSS) (Não será abordado)\n",
    "+ [JS](https://en.wikipedia.org/wiki/JavaScript) (Não será abordado)\n",
    "\n",
    "Após um melhor entedimento teórico utilizaremos algumas bibliotecas do Python para realizar essa tarefa como [`requests`](https://pypi.org/project/requests/) e [`bs4`](https://pypi.org/project/bs4/). Algumas importante, mas que não serão utilizadas: [`scrapy`](https://pypi.org/project/Scrapy/) e [`selenium`](https://pypi.org/project/selenium/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceitos básicos\n",
    "\n",
    "## HTML\n",
    "\n",
    "*HyperText Markup Language* (HTML) ou Linguagem de Marcação de Hipertexto é uma linguaguem que server para a construção de páginas web. Ela serve para informar ao navegador como exibir o conteúdo da página.\n",
    "\n",
    "O HTML é formado por tags, é importante compreendê-las e reconhecê-las já que fascilitam o trabalho de raspagem de dados na web. Algumas tags principais:\n",
    "- **html**: informar ao navegador aonde temos código em HTML\n",
    "- **head**: contêm informações como o título da página \n",
    "- **body**: é aonde o conteúdo principal da página está inserido, usualmente é onde o scraping ocorre.\n",
    "- **p**: delimita um parágrafo\n",
    "- **a**: para links\n",
    "- **div**: aponta uma região na página, útil para dividir o conteúdo\n",
    "- **b**: texto em negrito\n",
    "- **i**: texto em itálico\n",
    "- **table**: cria uma tablea\n",
    "- **form**: cria um formulário\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS\n",
    "\n",
    "*Cascading Style Sheets* (CSS)  é um mecanismo para adicionar estilos (cores, fontes, espaçamento, etc.) a um documento web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## JS\n",
    "*JavaScript* (JS) é uma linguagem de programação que adiciona interatividade às paginas web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On\n",
    "\n",
    "### Explicando como extraimos a explicação da Wikipedia\n",
    "<br>\n",
    "Primeiro precisamos importas as bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, precisamos obter o conteúdo da página (HTML, CSS, JS). Podemos utilizar o método `get`do módulo **requests** para a *url* desejada e atribuí-lo à variável 'page':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://en.wikipedia.org/wiki/Web_scraping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a conexão for bem sucedida teremos um status 200 para a página:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obter o conteúdo da página:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>Web scraping - Wikipedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Web_scraping\",\"wgTitle\":\"Web scraping\",\"wgCurRevisionId\":897184947,\"wgRevisionId\":897184947,\"wgArticleId\":2696619,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 Danish-language sources (da)\",\"Articles needing additional references from June 2017\",\"All articles needing additional references\",\"Articles needing additional references from October 2018\",\"Articles with limited geographic scope from October 2015\",\"USA-centric\",\"Articles to be split from July 2018\",\"All articles to be split'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.content[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que essa forma de exibição não é muito fácil e podemos utilizar o BeautifulSoup para nos auxiliar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível melhorar ainda mais usando o método `prettify`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar um seletor CSS para extrair somente o corpo do HTML, veja que agora já está mais fácil encontrar o primeiro parágrafo, basta localizar o < p> a esquerda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.find('body').prettify()) # é uma outra opção via find\n",
    "#soup.select(\"html body\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos selecionar o primeiro parágrafo utilizando o método `find` para localizar uma *tag* **HTML** e o `get_text` para extrair somente o texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('p').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que estamos próximos do nosso objetivo final, basta usar o método `find_all` que conseguiremos econtrat todas as tags **p**'s. Atente que os resultados serão retornados em uma lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.\n",
      "\n",
      "Web scraping a web page involves fetching it and extracting from it.[1][2] Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies and their URLs, to a list (contact scraping).\n",
      "\n",
      "Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup and, web data integration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('p')[:3]: print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumindo, poderíamos ter utilizado somente os seguintes comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.\n",
      "\n",
      "Web scraping a web page involves fetching it and extracting from it.[1][2] Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies and their URLs, to a list (contact scraping).\n",
      "\n",
      "Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup and, web data integration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Web_scraping\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "for item in soup.find_all('p')[:3]: print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fixar as ideias veja como seria simples retornar os dois primeiros parágrafo na wikipédia sobre HTML, CSS e JS em três passos:\n",
    "+ Lista de *url*'s\n",
    "+ Função para retornar o parágrafo\n",
    "+ Loop para aplicar a função a cada item na lista\n",
    "\n",
    "Passo 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://pt.wikipedia.org/wiki/HTML',\n",
       " 'https://pt.wikipedia.org/wiki/Cascading_Style_Sheets',\n",
       " 'https://pt.wikipedia.org/wiki/JavaScript']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paginas = [\n",
    "    'https://pt.wikipedia.org/wiki/HTML',\n",
    "    'https://pt.wikipedia.org/wiki/Cascading_Style_Sheets',\n",
    "    'https://pt.wikipedia.org/wiki/JavaScript'\n",
    "]\n",
    "\n",
    "paginas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML (abreviação para a expressão inglesa HyperText Markup Language, que significa Linguagem de Marcação de Hipertexto) é uma linguagem de marcação utilizada na construção de páginas na Web. Documentos HTML podem ser interpretados por navegadores. A tecnologia é fruto da junção entre os padrões HyTime e SGML.\n",
      "/nHyTime é um padrão para a representação estruturada de hipermídia e conteúdo baseado em tempo. Um documento é visto como um conjunto de eventos concorrentes dependentes de tempo (como áudio, vídeo, etc.), conectados por hiperligações. O padrão é independente de outros padrões de processamento de texto em geral.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retorna_n_paragrafos(url, n = 1):\n",
    "    '''\n",
    "    Essa função retorna os n primeiros parágrafos da url selecionada.\n",
    "    Possui os seguintes argumentos:\n",
    "    @url: a url desejada\n",
    "    @n: número de parágrafos para retornar, default = 1\n",
    "    '''\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    paragrafos = soup.find_all('p')[:n]\n",
    "    paragrafos_texto = []\n",
    "    for item in paragrafos: paragrafos_texto.append(item.get_text())\n",
    "    \n",
    "    return '/n'.join(paragrafos_texto) #concatenar elementos e separar por nova linha\n",
    "\n",
    "\n",
    "print(retorna_n_paragrafos('https://pt.wikipedia.org/wiki/HTML', n = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que a função funciona, o primeiro /n ocorre porque o primeiro parágrafo não tinha conteúdo.\n",
    "\n",
    "Passo 3 (criando o loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pt.wikipedia.org/wiki/HTML\n",
      "HTML (abreviação para a expressão inglesa HyperText Markup Language, que significa Linguagem de Marcação de Hipertexto) é uma linguagem de marcação utilizada na construção de páginas na Web. Documentos HTML podem ser interpretados por navegadores. A tecnologia é fruto da junção entre os padrões HyTime e SGML.\n",
      "/nHyTime é um padrão para a representação estruturada de hipermídia e conteúdo baseado em tempo. Um documento é visto como um conjunto de eventos concorrentes dependentes de tempo (como áudio, vídeo, etc.), conectados por hiperligações. O padrão é independente de outros padrões de processamento de texto em geral.\n",
      "\n",
      "https://pt.wikipedia.org/wiki/Cascading_Style_Sheets\n",
      "Cascading Style Sheets (CSS) é um mecanismo para adicionar estilo (cores, fontes, espaçamento, etc.) a um documento web[1].\n",
      "/nO código CSS pode ser aplicado diretamente nas tags ou ficar contido dentro das tags <style>. Também é possível, em vez de colocar a formatação dentro do documento, criar um link para um arquivo CSS que contém os estilos. Assim, quando se quiser alterar a aparência dos documentos vinculados a este arquivo CSS, basta modifica-lo[2].\n",
      "\n",
      "https://pt.wikipedia.org/wiki/JavaScript\n",
      "JavaScript , frequentemente abreviado como JS, é uma linguagem de programação interpretada de alto nível, caracterizada também,como dinâmica, fracamente tipada, prototype-based e multi-paradigma.[2] Juntamente com HTML e CSS, o JavaScript é uma das três principais tecnologias da World Wide Web. JavaScript permite páginas da Web interativas e, portanto, é uma parte essencial dos aplicativos da web. A grande maioria dos sites usa, e todos os principais navegadores têm um mecanismo JavaScript dedicado para executá-lo.\n",
      "/nÉ atualmente a principal linguagem para programação client-side em navegadores web. É também bastante utilizada do lado do servidor através de ambientes como o node.js. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pagina in paginas:\n",
    "    out = retorna_n_paragrafos(pagina, n = 2)\n",
    "    print(pagina + '\\n' + out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fim."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/30e9a2a8dc2cfd3358f58ee1715f92f8"
  },
  "gist": {
   "data": {
    "description": "Web scraping with Python",
    "public": true
   },
   "id": "30e9a2a8dc2cfd3358f58ee1715f92f8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
